{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23e51237-b5c7-4ce6-b16b-11d1fdab3850",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Properties\n",
    "properties = [\n",
    "    (1, 101, \"New York\", \"Apartment\", 250, \"active\"),\n",
    "    (2, 102, \"Los Angeles\", \"House\", 280, \"active\"),\n",
    "    (3, 103, \"Chicago\", \"Condo\", 190, \"inactive\"),\n",
    "    (4, 104, \"Houston\", \"House\", 180, \"active\"),\n",
    "    (5, 105, \"Phoenix\", \"Apartment\", 110, \"active\"),\n",
    "    (6, 106, \"Philadelphia\", \"Condo\", 180, \"inactive\"),\n",
    "    (7, 107, \"San Antonio\", \"House\", 150, \"active\"),\n",
    "    (8, 108, \"San Diego\", \"Apartment\", 130, \"active\"),\n",
    "    (9, 109, \"Dallas\", \"House\", 170, \"inactive\"),\n",
    "    (10, 110, \"San Jose\", \"Condo\", 295, \"active\"),\n",
    "    (11, 111, \"Austin\", \"House\", 160, \"active\"),\n",
    "    (12, 112, \"Jacksonville\", \"Apartment\", 100, \"inactive\"),\n",
    "    (13, 113, \"Fort Worth\", \"Condo\", 85, \"active\"),\n",
    "    (14, 114, \"Columbus\", \"House\", 140, \"active\"),\n",
    "    (15, 115, \"Charlotte\", \"Apartment\", 125, \"inactive\")\n",
    "]\n",
    "\n",
    "properties_df = spark.createDataFrame(properties, [\"property_id\", \"host_id\", \"location\", \"type\", \"price_per_night\", \"status\"])\n",
    "\n",
    "properties_df.write.mode(\"overwrite\").format(\"delta\").save(\"/Volumes/tabular/dataexpert/sirdonaldo/properties\")\n",
    "\n",
    "# Hosts\n",
    "hosts = [\n",
    "    (101, \"John Davies\", \"johndavies@gmail.com\", 6, 4.5),\n",
    "    (102, \"Jane Smith\", \"janesmith@yahoo.com\", 2, 4.6),\n",
    "    (103, \"Alice Johnson\", \"alicejohnson@hotmail.com\", 15, 4.3),\n",
    "    (104, \"Bob Brown\", \"bobbrown@gmail.com\", 8, 4.5),\n",
    "    (105, \"Charlie Davis\", \"charliedavies@gmail.com\", 4, 4.6),\n",
    "    (106, \"David Garcia\", \"davidgarcia@yahoo.com\", 3, 4.8),\n",
    "    (107, \"Eve Rodriguez\", \"everodriguez@hotmail.com\", 9, 4.5),\n",
    "    (108, \"Frank Martinez\", \"frankmartinez@live.com\", 4, 4.4),\n",
    "    (109, \"Grace Lee\", \"gracelee@tekluminate.net\", 7, 4.8),\n",
    "    (110, \"Hannah Kim\", \"hannahkim@gmail.com\", 7, 4.7),\n",
    "    (111, \"Ivan Nguyen\", \"ivannguyen@gmail.com\", 5, 4.6),\n",
    "    (112, \"Tosin Adarabioyo\", \"tosinadarabioyo@gmail.com\", 9, 4.9),\n",
    "    (113, \"Karen Hall\", \"karenhall@yahoo.com\", 4, 3.7),\n",
    "    (114, \"Liam Clark\", \"liamclark@nba.com\", 4, 4.3),\n",
    "    (115, \"Mia Patel\", \"miapatel@gmail.com\", 24, 4.1)\n",
    "]\n",
    "\n",
    "hosts_df = spark.createDataFrame(hosts, [\"host_id\", \"name\", \"contact_info\", \"number_of_properties\", \"rating\"])\n",
    "\n",
    "hosts_df.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"/Volumes/tabular/dataexpert/Sirdonaldo/hosts\"\n",
    ")\n",
    "\n",
    "# Guests\n",
    "guests = [\n",
    "    (201, \"Santan Dave\", \"santandave@ukrap.com\", 7, 6),\n",
    "    (202, \"Micah Welps\", \"micahwelps@yahoo.com\", 10, 4),\n",
    "    (203, \"Raj Patel\", \"rajpatel@gmail.com\", 12, 5),\n",
    "    (204, \"Sara Lee\", \"sara.lee@hotmail.com\", 2, 1),\n",
    "    (205, \"Tom Brown\", \"tombrown@gmail.com\", 3, 3),\n",
    "    (206, \"Lucy Johnson\", \"lucyjohnson@yahoo.com\", 5, 4),\n",
    "    (207, \"Emma Davis\", \"emmadavis@hotmail.com\", 8, 5),\n",
    "    (208, \"Oliver White\", \"oliverwhite@gmail.com\", 3, 2),\n",
    "    (209, \"Sophia Green\", \"sophiagreen@yahoo.com\", 4, 4),\n",
    "    (210, \"Noah Black\", \"noahblack@hotmail.com\", 6, 3),\n",
    "    (211, \"Ava Brown\", \"avabrown@gmail.com\", 9, 5),\n",
    "    (212, \"Ella White\", \"ellawhite@yahoo.com\", 11, 4),\n",
    "    (213, \"James Green\", \"jamesgreen@hotmail.com\", 13, 5),\n",
    "    (214, \"Isabella Black\", \"isabellaback@gmail.com\", 15, 4),\n",
    "    (215, \"William White\", \"williamwhite@yahoo.com\", 17, 5)\n",
    "]\n",
    "\n",
    "guests_df = spark.createDataFrame(guests, [\"guest_id\", \"name\", \"contact_info\", \"booking_history\", \"reviews_given\"])\n",
    "\n",
    "guests_df.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"/Volumes/tabular/dataexpert/Sirdonaldo/guests\"\n",
    ")\n",
    "\n",
    "# Bookings\n",
    "bookings = [\n",
    "    (301, 1, 201, \"2025-08-01\", \"2025-08-05\", 250*4, \"confirmed\"),   # New York, 4 nights\n",
    "    (302, 2, 202, \"2025-08-03\", \"2025-08-08\", 280*5, \"confirmed\"),   # Los Angeles, 5 nights\n",
    "    (303, 3, 203, \"2025-08-02\", \"2025-08-04\", 190*2, \"cancelled\"),   # Chicago, 2 nights\n",
    "    (304, 4, 204, \"2025-08-10\", \"2025-08-13\", 180*3, \"confirmed\"),   # Houston, 3 nights\n",
    "    (305, 5, 205, \"2025-08-12\", \"2025-08-16\", 110*4, \"confirmed\"),   # Phoenix, 4 nights\n",
    "    (306, 6, 206, \"2025-08-15\", \"2025-08-18\", 180*3, \"confirmed\"),   # Philadelphia, 3 nights\n",
    "    (307, 7, 207, \"2025-08-20\", \"2025-08-25\", 150*5, \"confirmed\"),   # San Antonio, 5 nights\n",
    "    (308, 8, 208, \"2025-08-21\", \"2025-08-23\", 130*2, \"cancelled\"),   # San Diego, 2 nights\n",
    "    (309, 9, 209, \"2025-08-22\", \"2025-08-24\", 170*2, \"confirmed\"),   # Dallas, 2 nights\n",
    "    (310, 10, 210, \"2025-08-28\", \"2025-08-31\", 295*3, \"confirmed\"),  # San Jose, 3 nights\n",
    "    (311, 11, 201, \"2025-09-01\", \"2025-09-04\", 160*3, \"confirmed\"),  # Austin, 3 nights\n",
    "    (312, 12, 202, \"2025-09-02\", \"2025-09-05\", 100*3, \"cancelled\"),  # Jacksonville, 3 nights\n",
    "    (313, 13, 203, \"2025-09-05\", \"2025-09-07\", 85*2, \"confirmed\"),   # Fort Worth, 2 nights\n",
    "    (314, 14, 204, \"2025-09-10\", \"2025-09-14\", 140*4, \"confirmed\"),  # Columbus, 4 nights\n",
    "    (315, 15, 205, \"2025-09-12\", \"2025-09-15\", 125*3, \"confirmed\")   # Charlotte, 3 nights\n",
    "]\n",
    "\n",
    "bookings_df = spark.createDataFrame(bookings, [\"booking_id\", \"property_id\", \"guest_id\", \"check_in_date\", \"check_out_date\", \"total_price\", \"status\"])\n",
    "\n",
    "bookings_df.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"/Volumes/tabular/dataexpert/Sirdonaldo/bookings\"\n",
    ")\n",
    "\n",
    "# Reviews \n",
    "reviews = [\n",
    "    (401, 301, 201, 1, 5, \"Amazing apartment in New York, very clean and central.\"),\n",
    "    (402, 302, 202, 2, 4, \"Great house in LA, host was responsive.\"),\n",
    "    (403, 303, 203, 3, 3, \"Chicago condo was okay, location good but noisy.\"),\n",
    "    (404, 304, 204, 4, 3, \"Houston stay was average, house needed maintenance.\"),\n",
    "    (405, 305, 205, 5, 5, \"Phoenix apartment was excellent and affordable.\"),\n",
    "    (406, 306, 206, 6, 4, \"Philadelphia condo was decent, Wi-Fi was slow.\"),\n",
    "    (407, 307, 207, 7, 5, \"San Antonio house was fantastic, great host!\"),\n",
    "    (408, 308, 208, 8, 2, \"San Diego apartment booking cancelled, host unhelpful.\"),\n",
    "    (409, 309, 209, 9, 2, \"Dallas house was disappointing, cleanliness issues.\"),\n",
    "    (410, 310, 210, 10, 5, \"San Jose condo was modern and comfortable.\"),\n",
    "    (411, 311, 201, 11, 4, \"Austin house was cozy, nice neighborhood.\"),\n",
    "    (412, 312, 202, 12, 3, \"Jacksonville apartment was basic, fair price.\"),\n",
    "    (413, 313, 203, 13, 3, \"Fort Worth condo was small, decent for short stays.\"),\n",
    "    (414, 314, 204, 14, 5, \"Columbus house was spacious and well-equipped.\"),\n",
    "    (415, 315, 205, 15, 4, \"Charlotte apartment was good value, would stay again.\")\n",
    "]\n",
    "\n",
    "reviews_df = spark.createDataFrame(reviews, [\"review_id\", \"booking_id\", \"guest_id\", \"property_id\", \"rating\", \"comments\"])\n",
    "\n",
    "reviews_df.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"/Volumes/tabular/dataexpert/Sirdonaldo/reviews\"\n",
    ")\n",
    "\n",
    "# Costs & Revenue\n",
    "costs_revenue = [\n",
    "    (501, 1,  \"maintenance\", 300, \"2025-08-01\"),\n",
    "    (502, 2,  \"cleaning\",    200, \"2025-08-02\"),\n",
    "    (503, 3,  \"utilities\",   150, \"2025-08-03\"),\n",
    "    (504, 4,  \"maintenance\", 250, \"2025-08-04\"),\n",
    "    (505, 5,  \"cleaning\",    100, \"2025-08-05\"),\n",
    "    (506, 6,  \"utilities\",   180, \"2025-08-06\"),\n",
    "    (507, 7,  \"maintenance\", 220, \"2025-08-07\"),\n",
    "    (508, 8,  \"cleaning\",    400, \"2025-08-08\"),\n",
    "    (509, 9,  \"utilities\",   160, \"2025-08-09\"),\n",
    "    (510, 10, \"maintenance\", 350, \"2025-08-10\"),\n",
    "    (511, 11, \"cleaning\",    180, \"2025-08-11\"),\n",
    "    (512, 12, \"utilities\",   120, \"2025-08-12\"),\n",
    "    (513, 13, \"maintenance\", 200, \"2025-08-13\"),\n",
    "    (514, 14, \"cleaning\",    170, \"2025-08-14\"),\n",
    "    (515, 15, \"utilities\",   140, \"2025-08-15\")\n",
    "]\n",
    "\n",
    "costs_revenue_df = spark.createDataFrame(costs_revenue, [\"cost_id\", \"property_id\", \"cost_type\", \"amount\", \"timestamp\"])\n",
    "\n",
    "costs_revenue_df.write.mode(\"overwrite\").format(\"delta\").save(\n",
    "    \"/Volumes/tabular/dataexpert/Sirdonaldo/costs_revenue\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77978dd-32c4-49e1-835f-b4bf1bf96f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>property_id</th><th>total_revenue</th><th>total_costs</th><th>profit</th></tr></thead><tbody><tr><td>2</td><td>1400</td><td>200</td><td>1200</td></tr><tr><td>1</td><td>1000</td><td>300</td><td>700</td></tr><tr><td>10</td><td>885</td><td>350</td><td>535</td></tr><tr><td>7</td><td>750</td><td>220</td><td>530</td></tr><tr><td>14</td><td>560</td><td>170</td><td>390</td></tr><tr><td>6</td><td>540</td><td>180</td><td>360</td></tr><tr><td>5</td><td>440</td><td>100</td><td>340</td></tr><tr><td>11</td><td>480</td><td>180</td><td>300</td></tr><tr><td>4</td><td>540</td><td>250</td><td>290</td></tr><tr><td>15</td><td>375</td><td>140</td><td>235</td></tr><tr><td>9</td><td>340</td><td>160</td><td>180</td></tr><tr><td>13</td><td>170</td><td>200</td><td>-30</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         1400,
         200,
         1200
        ],
        [
         1,
         1000,
         300,
         700
        ],
        [
         10,
         885,
         350,
         535
        ],
        [
         7,
         750,
         220,
         530
        ],
        [
         14,
         560,
         170,
         390
        ],
        [
         6,
         540,
         180,
         360
        ],
        [
         5,
         440,
         100,
         340
        ],
        [
         11,
         480,
         180,
         300
        ],
        [
         4,
         540,
         250,
         290
        ],
        [
         15,
         375,
         140,
         235
        ],
        [
         9,
         340,
         160,
         180
        ],
        [
         13,
         170,
         200,
         -30
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "property_id",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_revenue",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "total_costs",
            "nullable": false,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "profit",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 22
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "property_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_revenue",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "total_costs",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "profit",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- 1. Property Management\n",
    "-- Active vs Inactive Properties\n",
    "SELECT status, COUNT(*) AS property_count\n",
    "FROM delta.`/Volumes/tabular/dataexpert/sirdonaldo/properties`\n",
    "GROUP BY status;\n",
    "\n",
    "-- Top 5 properties by revenue\n",
    "SELECT b.property_id, SUM(b.total_price) AS total_revenue\n",
    "FROM delta.`/Volumes/tabular/dataexpert/sirdonaldo/bookings` b \n",
    "WHERE b.status = 'confirmed'\n",
    "GROUP BY b.property_id\n",
    "ORDER BY total_revenue DESC\n",
    "LIMIT 5;\n",
    "\n",
    "-- 2. Host Performance\n",
    "-- Host Ratings Ranking\n",
    "SELECT host_id, name, rating\n",
    "FROM delta.`/Volumes/tabular/dataexpert/sirdonaldo/hosts`\n",
    "ORDER BY rating DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- 3. Guest Booking Patterns\n",
    "-- Cancellation Rate\n",
    "SELECT\n",
    " (SUM(CASE WHEN status = 'cancelled' THEN 1 ELSE 0 END) * 100.0 / COUNT(*)) AS cancellation_rate\n",
    " FROM delta.`/Volumes/tabular/dataexpert/sirdonaldo/bookings`;\n",
    "\n",
    "-- Repeat Guests\n",
    "SELECT guest_id, COUNT(booking_id) AS total_bookings\n",
    "FROM delta.`/Volumes/tabular/dataexpert/sirdonaldo/bookings`\n",
    "WHERE status = 'confirmed' \n",
    "GROUP BY guest_id\n",
    "HAVING COUNT(booking_id) > 1;\n",
    "\n",
    "-- 4. Review Analysis\n",
    "-- Average Guest Satisfaction per Property\n",
    "SELECT property_id, AVG(rating) AS avg_rating\n",
    "FROM delta.`/Volumes/tabular/dataexpert/sirdonaldo/reviews`\n",
    "GROUP BY property_id;\n",
    "\n",
    "-- 5. Cost and Revenue Optimization\n",
    "-- Revenue vs Costs per Property\n",
    "SELECT \n",
    "  b.property_id,\n",
    "  SUM(b.total_price) AS total_revenue,\n",
    "  COALESCE(SUM(c.amount), 0) AS total_costs,\n",
    "  (SUM(b.total_price) - COALESCE(SUM(c.amount), 0)) AS profit\n",
    "FROM delta.`/Volumes/tabular/dataexpert/sirdonaldo/bookings` b\n",
    "LEFT JOIN delta.`/Volumes/tabular/dataexpert/sirdonaldo/costs_revenue` c\n",
    "  ON b.property_id = c.property_id\n",
    "WHERE b.status = 'confirmed'\n",
    "GROUP BY b.property_id\n",
    "ORDER BY profit DESC;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19e10b07-bd7b-407d-9286-51d25112301a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occupancy Rate\n9.11%\nAverage Booking Value\n+-----------------+\n|avg_booking_value|\n+-----------------+\n|623.3333333333334|\n+-----------------+\n\nHost Responsiveness (synthetic)\n+-------------------------+\n|avg_response_time_minutes|\n+-------------------------+\n|        44.86666666666667|\n+-------------------------+\n\nCancellation Rate\n20.00%\nGuest Satisfaction Score\n+------------------------+\n|guest_satisfaction_score|\n+------------------------+\n|                     3.8|\n+------------------------+\n\nRevenue Per Property\n498.67\nRepeat Booking Rate\n20.00%\nMaintenance Cost per Property\n208.00\nReview Sentiment Analysis\n0.33\nListing Activation Time (synthetic)\n+-------------------+\n|avg_activation_days|\n+-------------------+\n|                5.0|\n+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff, sum as _sum, count, col, when, lower, avg\n",
    "\n",
    "# 1. Occupancy Rate \n",
    "booked_nights = bookings_df.filter(bookings_df.status == \"confirmed\") \\\n",
    "    .withColumn(\"nights\", datediff(\"check_out_date\", \"check_in_date\")) \\\n",
    "    .agg(_sum(\"nights\")).first()[0]\n",
    "\n",
    "available_nights = properties_df.count() * 30  \n",
    "occupancy_rate = (booked_nights / available_nights) * 100\n",
    "print(\"Occupancy Rate\")\n",
    "print(f\"{occupancy_rate:.2f}%\")\n",
    "\n",
    "# 2. Average Booking Value\n",
    "avg_booking_value = bookings_df.filter(bookings_df.status == \"confirmed\").agg(\n",
    "    (_sum(\"total_price\") / count(\"booking_id\")).alias(\"avg_booking_value\")\n",
    ")\n",
    "print(\"Average Booking Value\")\n",
    "avg_booking_value.show()\n",
    "\n",
    "# 3. Host Responsiveness \n",
    "hosts_with_response = hosts_df.withColumn(\n",
    "    \"response_time_minutes\",\n",
    "    (col(\"rating\") * 10).cast(\"int\")   # fake values for demo\n",
    ")\n",
    "host_responsiveness = hosts_with_response.agg(\n",
    "    (avg(\"response_time_minutes\")).alias(\"avg_response_time_minutes\")\n",
    ")\n",
    "print(\"Host Responsiveness (synthetic)\")\n",
    "host_responsiveness.show()\n",
    "\n",
    "# 4. Cancellation Rate \n",
    "cancellations = bookings_df.filter(bookings_df.status == \"cancelled\").count()\n",
    "total_bookings = bookings_df.count()\n",
    "cancellation_rate = (cancellations / total_bookings) * 100\n",
    "print(\"Cancellation Rate\")\n",
    "print(f\"{cancellation_rate:.2f}%\")\n",
    "\n",
    "# 5. Guest Satisfaction Score \n",
    "guest_satisfaction = reviews_df.agg(\n",
    "    (_sum(\"rating\") / count(\"review_id\")).alias(\"guest_satisfaction_score\")\n",
    ")\n",
    "print(\"Guest Satisfaction Score\")\n",
    "guest_satisfaction.show()\n",
    "\n",
    "# 6. Revenue Per Property \n",
    "total_revenue = bookings_df.filter(bookings_df.status == \"confirmed\").agg(\n",
    "    _sum(\"total_price\")\n",
    ").first()[0]\n",
    "num_properties = properties_df.count()\n",
    "revenue_per_property = total_revenue / num_properties\n",
    "print(\"Revenue Per Property\")\n",
    "print(f\"{revenue_per_property:.2f}\")\n",
    "\n",
    "# 7. Repeat Booking Rate \n",
    "repeat_guests = bookings_df.filter(bookings_df.status == \"confirmed\") \\\n",
    "    .groupBy(\"guest_id\").count().filter(\"count > 1\").count()\n",
    "total_guests = guests_df.count()\n",
    "repeat_booking_rate = (repeat_guests / total_guests) * 100\n",
    "print(\"Repeat Booking Rate\")\n",
    "print(f\"{repeat_booking_rate:.2f}%\")\n",
    "\n",
    "# 8. Maintenance Cost per Property \n",
    "total_costs = costs_revenue_df.agg(_sum(\"amount\")).first()[0]\n",
    "num_properties = properties_df.count()\n",
    "maintenance_cost_per_property = total_costs / num_properties\n",
    "print(\"Maintenance Cost per Property\")\n",
    "print(f\"{maintenance_cost_per_property:.2f}\")\n",
    "\n",
    "# 9. Review Sentiment Analysis \n",
    "sentiment_df = reviews_df.withColumn(\n",
    "    \"sentiment\",\n",
    "    when(lower(col(\"comments\")).rlike(\"excellent|amazing|great|good\"), \"positive\")\n",
    "    .when(lower(col(\"comments\")).rlike(\"bad|poor|disappointing|dirty\"), \"negative\")\n",
    "    .otherwise(\"neutral\")\n",
    ")\n",
    "positive_reviews = sentiment_df.filter(col(\"sentiment\") == \"positive\").count()\n",
    "negative_reviews = sentiment_df.filter(col(\"sentiment\") == \"negative\").count()\n",
    "total_reviews = sentiment_df.count()\n",
    "sentiment_score = (positive_reviews - negative_reviews) / total_reviews\n",
    "print(\"Review Sentiment Analysis\")\n",
    "print(f\"{sentiment_score:.2f}\")\n",
    "\n",
    "# 10. Listing Activation Time \n",
    "properties_with_activation = properties_df.withColumn(\"activation_requested\", col(\"property_id\")*2) \\\n",
    "    .withColumn(\"activation_completed\", (col(\"property_id\")*2 + 5))\n",
    "\n",
    "activation_time = properties_with_activation.withColumn(\n",
    "    \"activation_days\", (col(\"activation_completed\") - col(\"activation_requested\"))\n",
    ")\n",
    "avg_activation_time = activation_time.agg(\n",
    "    (avg(\"activation_days\")).alias(\"avg_activation_days\")\n",
    ")\n",
    "print(\"Listing Activation Time (synthetic)\")\n",
    "avg_activation_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c6fdf86-62ba-4aef-8410-fd06aa17d252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "    <html>\n",
       "      <div style=\"font-size:18px\">\n",
       "        The Delta Live Tables (DLT) module is not supported on this cluster.\n",
       "        You should either <a href=\"?o=1352785079224954#joblist/pipelines/create?initialSource=%2FUsers%2Fdonaldsonajilore%40gmail.com%2FOlumuyiwa+Ajilore+Spark+Streaming+Assignment&redirectNotebookId=492587833878316\">create a new pipeline</a> or use an existing pipeline to run DLT code.\n",
       "      </div>\n",
       "    </html>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mDLTImportException\u001B[0m                        Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-6759857601255788>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col, \u001B[38;5;28msum\u001B[39m \u001B[38;5;28;01mas\u001B[39;00m _sum, count, datediff, avg, when, lower\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# =====================\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# BRONZE (raw tables)\u001B[39;00m\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# =====================\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/PostImportHook.py:231\u001B[0m, in \u001B[0;36m_ImportHookChainedLoader.exec_module\u001B[0;34m(self, module)\u001B[0m\n",
       "\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexec_module\u001B[39m(\u001B[38;5;28mself\u001B[39m, module):\n",
       "\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m--> 231\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader\u001B[38;5;241m.\u001B[39mexec_module(module)\n",
       "\u001B[1;32m    232\u001B[0m         notify_module_loaded(module)\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mImportError\u001B[39;00m, \u001B[38;5;167;01mAttributeError\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/dlt/__init__.py:22\u001B[0m\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# DLT python module is not supported when Spark Connect is enabled. So, we throw an error\u001B[39;00m\n",
       "\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# here.\u001B[39;00m\n",
       "\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_spark:\n",
       "\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DLTImportException(\n",
       "\u001B[1;32m     23\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelta Live Tables module is not supported on Spark Connect clusters. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     24\u001B[0m     )\n",
       "\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhelpers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deprecation_warn\n",
       "\n",
       "\u001B[0;31mDLTImportException\u001B[0m: Delta Live Tables module is not supported on Spark Connect clusters. "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "DLTImportException",
        "evalue": "Delta Live Tables module is not supported on Spark Connect clusters. "
       },
       "metadata": {
        "errorSummary": "<span class='ansi-red-fg'>DLTImportException</span>: Delta Live Tables module is not supported on Spark Connect clusters. "
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mDLTImportException\u001B[0m                        Traceback (most recent call last)",
        "File \u001B[0;32m<command-6759857601255788>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col, \u001B[38;5;28msum\u001B[39m \u001B[38;5;28;01mas\u001B[39;00m _sum, count, datediff, avg, when, lower\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# =====================\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# BRONZE (raw tables)\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# =====================\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/PostImportHook.py:231\u001B[0m, in \u001B[0;36m_ImportHookChainedLoader.exec_module\u001B[0;34m(self, module)\u001B[0m\n\u001B[1;32m    229\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexec_module\u001B[39m(\u001B[38;5;28mself\u001B[39m, module):\n\u001B[1;32m    230\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 231\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader\u001B[38;5;241m.\u001B[39mexec_module(module)\n\u001B[1;32m    232\u001B[0m         notify_module_loaded(module)\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mImportError\u001B[39;00m, \u001B[38;5;167;01mAttributeError\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/spark/python/dlt/__init__.py:22\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# DLT python module is not supported when Spark Connect is enabled. So, we throw an error\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# here.\u001B[39;00m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_remote_spark:\n\u001B[0;32m---> 22\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DLTImportException(\n\u001B[1;32m     23\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDelta Live Tables module is not supported on Spark Connect clusters. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     24\u001B[0m     )\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdlt\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mhelpers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deprecation_warn\n",
        "\u001B[0;31mDLTImportException\u001B[0m: Delta Live Tables module is not supported on Spark Connect clusters. "
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import col, sum as _sum, count, datediff, avg, when, lower\n",
    "\n",
    "# =====================\n",
    "# BRONZE (raw tables)\n",
    "# =====================\n",
    "\n",
    "@dlt.table\n",
    "def bronze_properties_sirdonaldo():\n",
    "    return spark.read.format(\"delta\").load(\"/Volumes/tabular/dataexpert/sirdonaldo/properties\")\n",
    "\n",
    "@dlt.table\n",
    "def bronze_hosts_sirdonaldo():\n",
    "    return spark.read.format(\"delta\").load(\"/Volumes/tabular/dataexpert/sirdonaldo/hosts\")\n",
    "\n",
    "@dlt.table\n",
    "def bronze_guests_sirdonaldo():\n",
    "    return spark.read.format(\"delta\").load(\"/Volumes/tabular/dataexpert/sirdonaldo/guests\")\n",
    "\n",
    "@dlt.table\n",
    "def bronze_bookings_sirdonaldo():\n",
    "    return spark.read.format(\"delta\").load(\"/Volumes/tabular/dataexpert/sirdonaldo/bookings\")\n",
    "\n",
    "@dlt.table\n",
    "def bronze_reviews_sirdonaldo():\n",
    "    return spark.read.format(\"delta\").load(\"/Volumes/tabular/dataexpert/sirdonaldo/reviews\")\n",
    "\n",
    "@dlt.table\n",
    "def bronze_costs_revenue_sirdonaldo():\n",
    "    return spark.read.format(\"delta\").load(\"/Volumes/tabular/dataexpert/sirdonaldo/costs_revenue\")\n",
    "\n",
    "\n",
    "# =====================\n",
    "# SILVER (cleaned data)\n",
    "# =====================\n",
    "\n",
    "@dlt.table\n",
    "def silver_confirmed_bookings_sirdonaldo():\n",
    "    return dlt.read(\"bronze_bookings_sirdonaldo\").filter(col(\"status\") == \"confirmed\")\n",
    "\n",
    "@dlt.table\n",
    "def silver_active_properties_sirdonaldo():\n",
    "    return dlt.read(\"bronze_properties_sirdonaldo\").filter(col(\"status\") == \"active\")\n",
    "\n",
    "\n",
    "# =====================\n",
    "# GOLD (business KPIs)\n",
    "# =====================\n",
    "\n",
    "@dlt.table\n",
    "def gold_occupancy_rate_sirdonaldo():\n",
    "    df = dlt.read(\"silver_confirmed_bookings_sirdonaldo\").withColumn(\n",
    "        \"nights\", datediff(\"check_out_date\", \"check_in_date\")\n",
    "    )\n",
    "    agg_df = df.groupBy().agg(_sum(\"nights\").alias(\"booked_nights\"))\n",
    "    total_properties = dlt.read(\"bronze_properties_sirdonaldo\").count()\n",
    "    available_nights = total_properties * 30\n",
    "    return agg_df.withColumn(\n",
    "        \"occupancy_rate\", (col(\"booked_nights\") / available_nights) * 100\n",
    "    ).select(\"occupancy_rate\")\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def gold_avg_booking_value_sirdonaldo():\n",
    "    df = dlt.read(\"silver_confirmed_bookings_sirdonaldo\")\n",
    "    return df.groupBy().agg(\n",
    "        (_sum(\"total_price\") / count(\"booking_id\")).alias(\"avg_booking_value\")\n",
    "    )\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def gold_host_responsiveness_sirdonaldo():\n",
    "    df = dlt.read(\"bronze_hosts_sirdonaldo\").withColumn(\n",
    "        \"response_time_minutes\", (col(\"rating\") * 10).cast(\"int\")\n",
    "    )\n",
    "    return df.groupBy().agg(avg(\"response_time_minutes\").alias(\"avg_response_time_minutes\"))\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def gold_cancellation_rate_sirdonaldo():\n",
    "    df = dlt.read(\"bronze_bookings_sirdonaldo\")\n",
    "    return df.groupBy().agg(\n",
    "        ( ( _sum(when(col(\"status\") == \"cancelled\", 1).otherwise(0)) / count(\"*\") ) * 100\n",
    "        ).alias(\"cancellation_rate\")\n",
    "    )\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def gold_guest_satisfaction_sirdonaldo():\n",
    "    df = dlt.read(\"bronze_reviews_sirdonaldo\")\n",
    "    return df.groupBy().agg(\n",
    "        (_sum(\"rating\") / count(\"review_id\")).alias(\"guest_satisfaction_score\")\n",
    "    )\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def gold_revenue_per_property_sirdonaldo():\n",
    "    df = dlt.read(\"silver_confirmed_bookings_sirdonaldo\")\n",
    "    return df.groupBy(\"property_id\").agg(\n",
    "        _sum(\"total_price\").alias(\"revenue_per_property\")\n",
    "    )\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def gold_repeat_booking_rate_sirdonaldo():\n",
    "    df = dlt.read(\"silver_confirmed_bookings_sirdonaldo\")\n",
    "    repeat_df = df.groupBy(\"guest_id\").count().filter(\"count > 1\")\n",
    "    total_guests = dlt.read(\"bronze_guests_sirdonaldo\").count()\n",
    "    rate = (repeat_df.count() / total_guests) * 100 if total_guests > 0 else 0\n",
    "    return spark.createDataFrame([(rate,)], [\"repeat_booking_rate\"])\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def gold_maintenance_cost_per_property_sirdonalo():\n",
    "    df = dlt.read(\"bronze_costs_revenue_sirdonaldo\")\n",
    "    return df.groupBy(\"property_id\").agg(\n",
    "        avg(\"amount\").alias(\"maintenance_cost_per_property\")\n",
    "    )\n",
    "\n",
    "\n",
    "@dlt.table\n",
    "def gold_review_sentiment_score_sirdonaldo():\n",
    "    df = dlt.read(\"bronze_reviews_sirdonaldo\").withColumn(\n",
    "        \"sentiment\",\n",
    "        when(lower(col(\"comments\")).rlike(\"excellent|amazing|great|good\"), \"positive\")\n",
    "        .when(lower(col(\"comments\")).rlike(\"bad|poor|disappointing|dirty\"), \"negative\")\n",
    "        .otherwise(\"neutral\")\n",
    "    )\n",
    "    return df.groupBy(\"property_id\").agg(\n",
    "        (_sum(when(col(\"sentiment\") == \"positive\", 1).otherwise(0)) -\n",
    "         _sum(when(col(\"sentiment\") == \"negative\", 1).otherwise(0))\n",
    "        ).alias(\"sentiment_score\")\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6012786233084582,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Olumuyiwa Ajilore Spark Streaming Assignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}